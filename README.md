# NoCode-bench Experiments

This repository contains records of submissions to the [NoCode-bench](https://github.com/ZJU-CTAG/NoCode-bench/) leaderboard.

<details>
<summary>How is this repository organized?</summary>
  
```
experiments/
â”œâ”€â”€ evaluation/
â”‚ â”œâ”€â”€ full/
â”‚ â””â”€â”€ verified/
|   â”œâ”€â”€ <date>_<model>
â”‚   â”‚ â”œâ”€â”€ evaluation_details.jsonl
â”‚   â”‚ â”œâ”€â”€ metadata.yaml
â”‚   â”‚ â”œâ”€â”€ README.md
â”‚   â”‚ â”œâ”€â”€ logs/<instance_id>/<exec. artifacts> (Execution Logs)
â”‚   â”‚ â””â”€â”€ trajs/*.traj (Reasoning Traces)
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

Top level directories in `evaluation/` are different splits of NoCode-bench (full, verified).
* Each subfolder is a submission to that benchmark.
* A subfolder contains the predictions, results, execution logs, and trajectories (if applicable) for the submission.

These logs are publicly accessible and meant to enable greater reproducibility and transparency of the experiments conducted on the NoCode-bench task.
</details>

## ğŸ” Viewing Logs, Trajectories
You can download the logs and trajectories for each submission by running the following command to download the data:
```bash
python analysis.download_logs.py \
    --logs \  # Download from Logs repo (use --trajs to download from Trajs repo instead)
    --target_folder test_case_1 \  # <name_of_the_folder_inside_repo_to_download>
    --local_dir ./downloads \  # <path_to_local_output_directory>
    --no_unzip  # (optional) Skip unzipping .zip files after download
```

Replace `--logs` with `--trajs` if you want to download trajectories instead of logs.
You can omit `--target_folder` to download all files from the selected repo.
Remove `--no_unzip` if you want zip files to be extracted automatically.

## ğŸ† Leaderboard Participation
To evaluate on NoCode-bench, check out the [main repository](https://github.com/ZJU-CTAG/NoCode-bench) for instructions.
Please follow these instructions carefully to ensure your submission is merged on time!

### NoCode-bench [Full, Verified]
1. Fork this repository
2. Under the split that you evaluate on (e.g. `evaluation/verified/`), create a new folder with the submission date and the model name (e.g. `20250722_openhands_gpt4`).
3. Within the folder (`evaluation/<split>/<date + model>`), please provide the following:

<details>
<summary>ğŸ“‹ Required Assets</summary>
<br>

  * `evaluation_details.jsonl`: Evaluation results on NoCode-bench.
  * `metadata.yaml`: See `checklist.md`
  * `README.md`: See `checklist.md`
  * `trajs/`: Reasoning traces reflecting how your system solved each task instance (see below for more details)
  * `logs/`: NoCode-bench evaluation artifacts dump
    - **NOTE**: You shouldn't have to create any of these files. They should automatically be generated by NoCode-bench evaluation.
</details>

4. Create a pull request to this repository with the new folder.

## âœ… Result Verification
If you are interested in receiving the "verified" checkmark on your submission, please do the following:
1. Create an issue
2. In the issue, provide us instructions on how to run your model on NoCode-bench.
3. We will run your model on a random subset of NoCode-bench and verify the results.

## ğŸ“ Contact
Questions? Please create an issue or email to **dengle@zju.edu.cn**, **zhonghao.j@zju.edu.cn** or **liu_zx@zju.edu.cn**.

## âœï¸ Citation
If you found this repository helpful or are citing the numbers on the leaderboard for academic purposes, please use cite [NoCode-bench](https://arxiv.org/pdf/2507.18130).
```
@misc{deng2025nocodebenchbenchmarkevaluatingnatural,
      title={NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition}, 
      author={Le Deng and Zhonghao Jiang and Jialun Cao and Michael Pradel and Zhongxin Liu},
      year={2025},
      eprint={2507.18130},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2507.18130}, 
}
```

## ğŸ“œ Acknowledgements
This repository partly depends on [SWE-bench](https://swe-bench.github.io/) project.
