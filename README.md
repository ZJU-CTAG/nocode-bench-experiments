# NoCode-bench Experiments

This repository contains records of submissions to the [SWE-bench](https://swe-bench.github.io/) leaderboard.

<details>
<summary>How is this repository organized?</summary>
  
```
experiments/
â”œâ”€â”€ evaluation/
â”‚ â”œâ”€â”€ full/
â”‚ â””â”€â”€ verified/
|   â”œâ”€â”€ <date>_<model>
â”‚   â”‚ â”œâ”€â”€ evaluation_details.jsonl
â”‚   â”‚ â”œâ”€â”€ metadata.yaml
â”‚   â”‚ â”œâ”€â”€ README.md
â”‚   â”‚ â”œâ”€â”€ logs/<instance_id>/<exec. artifacts> (Execution Logs)
â”‚   â”‚ â””â”€â”€ trajs/*.traj (Reasoning Traces)
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

Top level directories in `evaluation/` are different splits of NoCode-bench (full, verified).
* Each subfolder is a submission to that benchmark.
* A subfolder contains the predictions, results, execution logs, and trajectories (if applicable) for the submission.

These logs are publicly accessible and meant to enable greater reproducibility and transparency of the experiments conducted on the NoCode-bench task.
</details>

## ğŸ” Viewing Logs, Trajectories
You can download the logs and trajectories for each submission by running the following command to download the data:
```bash
python -m analysis.download_logs evaluation/<split>/<date + model>
python -m analysis.download_logs evaluation/lite/20231010_rag_claude2
```
Logs and trajectories are saved to a public S3 Bucket. *You need an AWS account to download the logs and trajectories*. Namely, you'll need to create an [AWS account](https://aws.amazon.com/), download the [AWS CLI](https://aws.amazon.com/cli/), and [configure the CLI with your credentials](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html).

## ğŸ† Leaderboard Participation
To evaluate on NoCode-bench, check out the [main repository]() for instructions.
Please follow these instructions carefully to ensure your submission is merged on time!

### NoCode-bench [Full, Verified]
1. Fork this repository
2. Under the split that you evaluate on (e.g. `evaluation/verified/`), create a new folder with the submission date and the model name (e.g. `20250722_openhands_gpt4`).
3. Within the folder (`evaluation/<split>/<date + model>`), please provide the following:

<details>
<summary>ğŸ“‹ Required Assets</summary>
<br>

  * `evaluation_details.jsonl`: Evaluation results on NoCode-bench.
  * `metadata.yaml`: See `checklist.md`
  * `README.md`: See `checklist.md`
  * `trajs/`: Reasoning traces reflecting how your system solved each task instance (see below for more details)
  * `logs/`: NoCode-bench evaluation artifacts dump
    - **NOTE**: You shouldn't have to create any of these files. They should automatically be generated by NoCode-bench evaluation.
</details>

4. Create a pull request to this repository with the new folder.

## âœ… Result Verification
If you are interested in receiving the "verified" checkmark on your submission, please do the following:
1. Create an issue
2. In the issue, provide us instructions on how to run your model on NoCode-bench.
3. We will run your model on a random subset of NoCode-bench and verify the results.

## ğŸ“ Contact
Questions? Please create an issue. 

## âœï¸ Citation
If you found this repository helpful or are citing the numbers on the leaderboard for academic purposes, please use cite [NoCode-bench]().

## ğŸ“œ Acknowledgements
This repository is part of the [SWE-bench](https://swe-bench.github.io/) project, and the leaderboard is a modified version of SWE-bench GitHub Pages.
